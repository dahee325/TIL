# 5-1 트리 알고리즘
```python
# 데이터 불러오기
import pandas as pd
red_wine = pd.read_csv('data/winequality-red.csv', sep=';')
white_wine = pd.read_csv('data/winequality-white.csv', sep=';')

# 필요한 데이터 추출, 생성
red_wine = red_wine[['alcohol', 'residual sugar', 'pH']]
red_wine.rename(columns={'residual sugar': 'sugar'}, inplace=True)
red_wine['class'] = 0

white_wine = white_wine[['alcohol', 'residual sugar', 'pH']]
white_wine.rename(columns={'residual sugar': 'sugar'}, inplace=True)
white_wine['class'] = 1

wine = pd.concat([red_wine, white_wine]) # concat() : 두개의 데이터 프레임을 위아래로 연결
# wine.info()

# 데이터세트 분리
data = wine[['alcohol', 'sugar', 'pH']]
target = wine[['class']]

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target)

# 정규화
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# 로지스틱 회귀
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

print(lr.coef_, lr.intercept_)
```
 ## 결정 트리
 - `DecisionTreeClassifier()` : 결정트리(스무고개와 같음), 비선형
 ```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier() 
# dt.fit(train_scaled, train_target)
dt.fit(train_input, train_target)

# print(dt.score(train_scaled, train_target))
# print(dt.score(test_scaled, test_target))
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
 ```
 
 ### 결정트리 시각화
 - `plt.figure()` : plot의 크기 지정
 - `plot_tree()` : 트리 그림 출력
 - 지니 불순도(gini) : 데이터 트리가 얼마나 한쪽으로 편향되어있는지, 낮을수록 좋음
```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10, 7)) 
plot_tree(dt, max_depth=1) # max_depth=1 : 트리 구조의 상단만 출력
plt.show()
```

### 가지치기
- `DecisionTreeClassifier()`의 옵션 `max_depth=` : 최대 몇개의 노드를 출력할지 지정
```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth=3) # max_depth=3으로 주니까 과대적합은 해결, 정확도는 떨어짐
# dt.fit(train_scaled, train_target)
dt.fit(train_input, train_target)

# print(dt.score(train_scaled, train_target))
# print(dt.score(test_scaled, test_target))
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
```

# 5-2
## 교차 검증과 그리드 서치
- 훈련 세트 60%, 검증 세트 20%, 테스트 세트 20%
```python
# 데이터 불러오기
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine_csv_data')
wine.head()

# 필요한 데이터 추출하기
data = wine[['alcohol', 'sugar', 'pH']]
target = wine[['class']]

# 훈련 세트와 테스트 세트 분리하기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target)

# 훈련 세트와 검증 세트 분리하기
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target)

# 결정 트리
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))

print(dt.score(test_input, test_target))
```

### 교차검증
- 검증 세트를 떼어 내어 평가하는 과정을 여러번 반복
- `cross_validate()` : 기본적으로 5-폴드 교차검증
    - 훈련 세트를 섞어서 폴드를 나누지 않음 => `cv=Stratified(Fold())` : 섞어서 나눔
    - `n_splits=` : 몇 폴드 교차검증을 할지
```python
from sklearn.model_selection import cross_validate

scores = cross_validate(dt, train_input, train_target)
print(scores)
```

### 그리드 서치
- `GridSearchCV()` : 하이퍼파라미터 탐색과 교차 검증을 한번에 수행
  - `n_jobs=` : 매개변수에서 병렬 실행에 사용할 CPU 코어 수 지정, 기본값 1,\
      -1로 지정하면 시스템에 있는 모든 코어 사용
- `params={}` : 그리드 서치 객체
    - `min_impurity_decrease` : 검증하고싶은 하이퍼 파라미터를 딕셔너리 키 값에 넣음
    - `max_depth` : 트리의 깊이 제한
    - `min_samples_split` : 노드를 나누기 위한 최소 샘플 수
1. 탐색할 매개변수 지정
2. 훈련 세트에서 그리드 서치를 수행하여 최상의 평균 검증 점수가 나오는 매개변수 조합 찾기 => 찾은 조합은 서치 객체에 저장
3. 그리드 서치는 최상의 매개변수에서 (교차 검증에서 사용한 훈련 세트가 아니라)전체 훈련 세트를 사용해 최종 모델 훈련 => 최종 모델 그리드 서치 객체에 저장
```python
from sklearn.model_selection import GridSearchCV

params = {
    'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004] # test 해보고싶은 파라미터를 다 넣음
}

gs = GridSearchCV(dt, params, n_jobs=-1)

gs.fit(train_input, train_target)
gs.best_estimator_ # 어떤 값을 넣었더니 성능이 가장 좋았어?
```

# 5-3
## 트리와 앙상블(ensemble)
- 정형 데이터 : 엑셀, CSV 등 숫자 데이터
- 비정형 데이터 : 텍스트 데이터, 사진, 음악 등
```python
import pandas as pd

df = pd.read_csv('https://bit.ly/wine_csv_data')

data = df[['alcohol', 'sugar', 'pH']]
target = df[['class']]

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target)
```

### 랜덤 포레스트(Random Forest)
- `RandomForestClassifier()` : 결정 트리를 랜덤하게 만들어 결정트리(기본적으로 100그루)의 숲을 만들고 각 결정트리의 예측을 사용해 최종 예측
- 부트스트랩 샘플(bootstrap sample) : 훈련 데이터에서 랜덤하게 추출한 샘플(중복 추출 가능)
- `feature_importances_` : 특성 중요도, 각 결정 트리의 특성 중요도를 취합한 것 => 과대적합 줄이고 일반화 성능 높임
- OOB(out of bag) : 부트스트랩 샘플이 포함되지 않고 남는 샘플

```python
# 교차검증
from sklearn.model_selection import cross_validate
# 랜덤포레스트
from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트

rf = RandomForestClassifier(n_jobs=-1)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=1)
# return_train_score=True : 기본값 False, 검증 점수 뿐만 아니라 훈련 세트에 대한 점수도 같이 반환
print(scores)

rf.fit(train_input, train_target)
rf.feature_importances_

rf = RandomForestClassifier(oob_score=True, n_jobs=1) # oob_score=True : 기본값 False, oob점수 평균 출력
rf.fit(train_input, train_target)
rf.oob_score_ # oob샘플을 사용하여 부트스트랩 샘플로 훈련한 결정트리 평가 => 검증 세트의 역할
```

### 엑스트라 트리(Extra Trees)
- `ExtraTreesClassifier()` : 랜덤 포레스트와 비슷하게 동작
- 부트스트랩 샘플 사용 X
- 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할 => 과대적합을 막고 검증 세트의 점수를 높이는 효과, 계산속도 빠름
```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1)

scores = cross_validate(et, train_input, train_target, return_train_score=True)
print(scores)
```

### 그레이디언트 부스팅(Gradient boosting)
- `GradientBoostingClassifier()` : 깊이가 얕은 결정트리 사용 => 과대적합에 강하고 높은 일반화 성능 기대
- 기본적으로 깊이가 3인 결정트리를 100개 사용
    - `n_estimators=` : 결정 트리의 개수 지정
- 결정트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동(경사 하강법을 사용하여 앙상블에 트리를 추가하는 방식)\
=> 분류 : 로지스틱 손실 함수, 회귀 : 평균제곱오차(MSE) 함수
```python
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=500) # n_estimators= : 결정 트리의 개수 지정
scores = cross_validate(gb, train_input, train_target, return_train_score=True)
print(scores)
```

### 히스토그램 기반 그레이디언트 부스팅
- `HistGradientBoostingClassifier()` : 입력 특성을 256개 구간으로 나눔 => 최적의 분할을 매우 빠르게 찾을 수 있음
- `permutation_importance()` : 히스토그램 기반 그레이디언트 부스팅의 특성 중요도 계산
```python
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier()
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
# return_train_score=True : 기본값 False, 검증 점수 뿐만 아니라 훈련 세트에 대한 점수도 같이 반환
print(scores)

from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target) # 하나씩 섞었을 때 변화량을 봐야하기 때문에 학습을 시킨 후 실행해야함
scores = permutation_importance(hgb, train_input, train_target)
print(scores.importances_mean)
```