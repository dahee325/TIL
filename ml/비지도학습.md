# 6-1 비지도학습
- 타깃이 없을 떄 데이터에 있는 패턴을 찾거나 데이터 구조를 파악하는 머신러닝 방식
- 알고리즘 스스로 데이터가 어떻게 구성되어 있는지 분석
## 군집 알고리즘(Clustering)
- 비슷한 샘플끼리 그룹(cluster)으로 모음
```python
import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('data/fruits_300.npy')
fruits.shape # 3차원, 100*100인 사진이 300장 있음

fruits[0, 0] # [첫번째 사진, 맨 윗줄]
```
- `plt.imshow()` : 이미지 출력 
    - `cmap=''` : 색 지정, 'gray_r'로 지정하면 다시 반전시켜 우리 눈에 보기 좋게 출력
```python
plt.imshow(fruits[0], cmap='gray_r') 
plt.show()
```
- `subplots()` : 여러개의 그래프를 배열처럼 쌓음, 한 화면에 그래프 두개 그리기
```python
fig, axs = plt.subplots(1, 2)
axs[0].imshow(fruits[100])
axs[1].imshow(fruits[200])
plt.show()
```

### 픽셀값 분석하기
- `reshape()` : 형태 바꾸기, 첫번째 차원을 -1로 지정하면 자동으로 남은 차원 할당(100개)
- 2차원 배열의 사진을 1차원 배열로 만들기 => 이미지로 출력하기는 어렵지만 배열을 계산할 때 편리
```python
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)

apple.shape

# 픽셀의 평균값
apple.mean(axis=1) # axis=0 : 행, axis=1 : 열
```
- `plt.hist()` : 히스토그램 그래프
    - `alpha=` : 1보다 작게하면 투명도를 줄 수 있음
```python
plt.hist(np.mean(apple, axis=1))
plt.hist(np.mean(pineapple, axis=1))
plt.hist(np.mean(banana, axis=1))
plt.show()
```
- 픽셀별 평균값
```python
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
```
- 픽셀별 평균값을 이미지로 출력
```python
apple_mean = np.mean(apple, axis=0).reshape(100, 100) # 1차원으로 만든 사진을 다시 2차원으로 만들기 => 시각화를 위해서
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```

### 평균값과 가까운 사진 고르기
```python
apple_mean = np.mean(apple, axis=0).reshape(100, 100) # 1차원으로 만든 사진을 다시 2차원으로 만들기 => 시각화를 위해서
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

abs_diff = np.abs(fruits - apple_mean)
# print(abs_diff)
abs_mean = np.mean(abs_diff, axis=(1, 2))
```
- `np.argsort()` : 정렬되기 전 인덱스 번호 저장
```python
apple_index = np.argsort(abs_mean)[0:100] # abs_mean의 데이터가 오름차순으로 정렬

fig, axs = plt.subplots(10, 10, figsize=(10, 10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray')
        axs[i, j].axis('off') # 좌표축 그리지 않기

plt.show()
```

# 6-2
## K-평균
- `KMeans()` : K-평균 군집 알고리즘이 평균값을 자동으로 찾아줌
    - `n_clusters=` : 군집 개수 지정
- 평균값(=클러스터 중심(cluster center), 센트로이드(centroid)) => 클러스터의 중심에 위치하기 때문
1. 무작위로 k개의 클러스터 중심 지정
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심 변경
4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복
```python
import numpy as np

fruits = np.load('data/fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state = 42)
km.fit(fruits_2d)

km.labels_ # => 레이블값 0, 1, 2
```

### 클러스터 중심
- KMeans 클래스가 최종적으로 찾은 클러스터 중심은 `cluster_centers_`속성에 저장되어있음
```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```

### 최적의 k 찾기
- 엘보우(elbow) 방법 : 적절한 클러스터 개수를 찾기 위한 대표적인 방법\
=> 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법\
=> 이너셔 그래프에서 감소하는 속도가 꺾이는 지점이 최적의 클러스터 개수
- `inertia_` : 이너셔(inertia) => 클러스터 중심과 클러스터에 속한 샘플 사이의 거리의 제곱합
```python
inertia = []
for k in range(2, 7):
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)
plt.plot(range(2, 7), inertia) # x : k, y : inertia
plt.show() # => k=3일 때 기울기가 바뀜
```

# 6-3
## 주성분 분석(PCA, principal component analysis)
- 대표적인 차원(특성) 축소 알고리즘
- `PCA()` : 최대한 특성을 살리면서 차원을 낮추는 방법
    - `n_components=` : 주성분의 개수 지정, 0 ~ 1 사이의 실수를 입력하면 지정된 비율에 도달할 때까지 자동으로 주성분을 찾음
```python
import numpy as np
fruits = np.load('data/fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.decomposition import PCA

pca = PCA(n_components=50)
pca.fit(fruits_2d)

pca.components_.shape
```
- 주성분 그림으로 표현
```python
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1): # 그림그리기
    n = len(arr)

    rows = int(np.ceil(n/10))
    cols = n if rows < 2 else 10

    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)

    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()

draw_fruits(pca.components_.reshape(-1, 100, 100))
```
- `pca.transform()` : `PCA(n_components=`에서 지정한 차원대로 원본 데이터의 차원을 줄여줌
```python
fruits_pca = pca.transform(fruits_2d)
fruits_pca.shape
```

### 원본 데이터 재구성(복원)
- `inverse_transform()` : 원본 데이터 복원 함수
```python
fruits_inverse = pca.inverse_transform(fruits_pca) 
f = fruits_inverse.reshape(-1, 100, 100)
draw_fruits(f[100:200])
```

### 설명된 분산
- 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값
- `pca.explained_varuiance_ratio_` : 각 주성분의 설명된 분산 비율
```python
np.sum(pca.explained_variance_ratio_)
```

### 다른 알고리즘과 함께 사용하기
1. 로지스틱 회귀
```python
# 로지스틱 회귀
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
target = np.array([0]*100 + [1]*100 + [2]*100)
# 0 : 사과, 1 : 파인애플, 2 : 바나나

# 교차검증
from sklearn.model_selection import cross_validate
scores = cross_validate(lr, fruits_2d, target)
print(scores)

scores = cross_validate(lr, fruits_pca, target)
print(scores)

# 주성분 분석
pca = PCA(n_components=0.5) # 설명된 분산의 50%에 달하는 주성분 찾기
pca.fit(fruits_2d)
pca.n_components_ # 2개의 특성만으로 원본데이터에 있는 분산의 50%를 설명할 수 있음

# 주성분 분석 결과 모델로 원본 데이터 변환
fruits_pca = pca.transform(fruits_2d)
fruits_pca.shape

# 교차검증 결과 확인
fruits_pca = pca.transform(fruits_2d)
fruits_pca.shape
```

2. K-평균 알고리즘
```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3)
km.fit(fruits_pca)

km.labels_ # => 0, 1, 2
```
- 시각화
```python
for label in range(3):
    data = fruits_pca[km.labels_ == label]
    plt.scatter(data[:, 0], data[:, 1])
```