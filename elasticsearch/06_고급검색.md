# 6-1. 한글 형태소 분석기 (nori)
## install
[Korean (nori) analysis plugin | Elastic Documentation](https://www.elastic.co/docs/reference/elasticsearch/plugins/analysis-nori)

```bash
# 위치 이동
cd elasticsearch-8.18.0/

# 설치
bin/elasticsearch-plugin install analysis-nori
```

- 하나의 토크나이저와 세 개의 토큰 필터로 구성
    - nori_tokenizer : 토크나이저
    - nori_part_of_speech : 토큰 필터
    - nori_readingform : 토큰 필터
    - nori_number : 토큰 필터

## 6-1-1. nori_tokenizer 토크나이저
```bash
# 전에 생성한 article이 있으면 지우기
DELETE /article

# 인덱스 생성
PUT /article
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "nori_tokenizer",
          "filter": [
            "nori_part_of_speech",
            "nori_readingform"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "content": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}
```

- 테스트 1
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "안녕하세요"
}
```
```bash
{
  "tokens": [
    {
      "token": "안녕",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    }
  ]
}
```
⇒ 하세요는 안녕을 꾸며주는 말이므로 안녕만 남김

- 테스트 2
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "잠실역에서 롯데타워가 보여요"
}
```
```bash
{
  "tokens": [
    {
      "token": "잠실",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "역",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 1
    },
    {
      "token": "롯데",
      "start_offset": 6,
      "end_offset": 8,
      "type": "word",
      "position": 3
    },
    {
      "token": "타워",
      "start_offset": 8,
      "end_offset": 10,
      "type": "word",
      "position": 4
    },
    {
      "token": "보이",
      "start_offset": 12,
      "end_offset": 15,
      "type": "word",
      "position": 6
    }
  ]
}
```
⇒ 핵심단어만 뽑아서 저장

- 테스트 3
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "세종시"
}
```
```bash
{
  "tokens": [
    {
      "token": "종시",
      "start_offset": 1,
      "end_offset": 3,
      "type": "word",
      "position": 1
    }
  ]
}
```
⇒ 하나의 단어로 봐야하는데 쪼개짐 → 문제점 1

- 테스트 4
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "알잘딱깔센"
}
```
```bash
{
  "tokens": [
    {
      "token": "알",
      "start_offset": 0,
      "end_offset": 1,
      "type": "word",
      "position": 0
    },
    {
      "token": "까",
      "start_offset": 3,
      "end_offset": 4,
      "type": "word",
      "position": 3
    },
    {
      "token": "센",
      "start_offset": 4,
      "end_offset": 5,
      "type": "word",
      "position": 5
    }
  ]
}
```
⇒ 하나의 단어로 봐야하는데 쪼개짐 → 신조어는 반영이 안되는 듯 → 문제점 2

- `decompound_mode` : 복합명사를 처리하는 방식 설정
    - `none` : 분해 없음, `가곡역`
    - `discard` : 화합물을 분해하고 원래 형태를 버림, `가곡역 → 가곡, 역`
    - `mixed` : 화합물을 분해하여 원래 형태 유지, `가곡역 → 가곡역, 가곡, 역`

- user_dictionary : 사용자 사전 지정
    - 세종 말뭉치와 mecab-ko-dic 사전 사용
    - `세종시 → 세종, 시`로 나눠야한다고 알려줘야함

- `elasticsearch-8.18.0/config/custom/user_dict.txt` 폴더와 파일 생성
```bash
세종시 세종 시
알잘딱깔센
```
⇒ `elasticsearch` 껐다 다시 켜기

- 옵션 넣어서 인덱스 생성
```bash
# 지우고 다시 만들기
DELETE /article

# 먼저 커스텀하고 my_analyzer에 넣음
PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                "user_dictionary": "custom/user_dict.txt"
            }
        },
        "filter": {},
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer"
            }
        }
    }
  }
}
```

- 테스트 1
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "안녕하세요"
}
```
```bash
{
  "tokens": [
    {
      "token": "안녕",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "하",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 1
    },
    {
      "token": "세요",
      "start_offset": 3,
      "end_offset": 5,
      "type": "word",
      "position": 2,
      "positionLength": 2
    },
    {
      "token": "시",
      "start_offset": 3,
      "end_offset": 5,
      "type": "word",
      "position": 2
    },
    {
      "token": "어요",
      "start_offset": 3,
      "end_offset": 5,
      "type": "word",
      "position": 3
    }
  ]
}
```

- 테스트 2
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "잠실역에서 롯데타워가 보여요"
}
```
```bash
{
  "tokens": [
    {
      "token": "잠실역",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0,
      "positionLength": 2
    },
    {
      "token": "잠실",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "역",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 1
    },
    {
      "token": "에서",
      "start_offset": 3,
      "end_offset": 5,
      "type": "word",
      "position": 2
    },
    {
      "token": "롯데",
      "start_offset": 6,
      "end_offset": 8,
      "type": "word",
      "position": 3
    },
    {
      "token": "타워",
      "start_offset": 8,
      "end_offset": 10,
      "type": "word",
      "position": 4
    },
    {
      "token": "가",
      "start_offset": 10,
      "end_offset": 11,
      "type": "word",
      "position": 5
    },
    {
      "token": "보여요",
      "start_offset": 12,
      "end_offset": 15,
      "type": "word",
      "position": 6,
      "positionLength": 2
    },
    {
      "token": "보이",
      "start_offset": 12,
      "end_offset": 15,
      "type": "word",
      "position": 6
    },
    {
      "token": "어요",
      "start_offset": 12,
      "end_offset": 15,
      "type": "word",
      "position": 7
    }
  ]
}
```

- 테스트 3
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "세종시"
}
```
```bash
{
  "tokens": [
    {
      "token": "세종시",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0,
      "positionLength": 2
    },
    {
      "token": "세종",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "시",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 1
    }
  ]
}
```

- 테스트 4
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "알잘딱깔센"
}
```
```bash
{
  "tokens": [
    {
      "token": "알잘딱깔센",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 0
    }
  ]
}
```

- user_dictionary_rules 사용 → 지정하는 단어가 몇개 안될 때 사용
```bash
# 지우고 다시 만들기
DELETE /article

PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                // "user_dictionary": "custom/user_dict.txt",
                "user_dictionary_rules": [
                    "c++",
                    "씨쁠쁠"
                ]
            }
        },
        "filter": {},
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer"
            }
        }
    }
  }
}
```

- 테스트 1
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "c++이 파이썬보다 더 어려워요"
}
```
```bash
{
  "tokens": [
    {
      "token": "c++",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "이",
      "start_offset": 3,
      "end_offset": 4,
      "type": "word",
      "position": 1
    },
    {
      "token": "파이썬",
      "start_offset": 5,
      "end_offset": 8,
      "type": "word",
      "position": 2
    },
    {
      "token": "보다",
      "start_offset": 8,
      "end_offset": 10,
      "type": "word",
      "position": 3
    },
    {
      "token": "더",
      "start_offset": 11,
      "end_offset": 12,
      "type": "word",
      "position": 4
    },
    {
      "token": "어려워요",
      "start_offset": 13,
      "end_offset": 17,
      "type": "word",
      "position": 5,
      "positionLength": 2
    },
    {
      "token": "어렵",
      "start_offset": 13,
      "end_offset": 17,
      "type": "word",
      "position": 5
    },
    {
      "token": "어요",
      "start_offset": 13,
      "end_offset": 17,
      "type": "word",
      "position": 6
    }
  ]
}
```

- 테스트 2
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "씨쁠쁠"
}
```
```bash
{
  "tokens": [
    {
      "token": "씨쁠쁠",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    }
  ]
}
```

## 6-1-2. nori_part_of_speech 토큰 필터
```bash
# 지우고 다시 만들기
DELETE /article

# 먼저 커스텀하고 my_analyzer에 넣음
PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                //"user_dictionary": "custom/user_dict.txt"
                "user_dictionary_rules": [
                    "c++",
                    "씨쁠쁠"
                ]
            }
        },
        "filter": {
            "my_filter": {
                "type": "nori_part_of_speech",
                "stoptags" :[
                    "IC"
                ]
            }
        },
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer",
                "filter": ["my_filter"]
            }
        }
    }
  }
}
```

- 테스트
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "우와! 잠실역에서 롯데타워가 보여요"
}
```
```bash
{
  "tokens": [
    {
      "token": "잠실역",
      "start_offset": 4,
      "end_offset": 7,
      "type": "word",
      "position": 1,
      "positionLength": 2
    },
    {
      "token": "잠실",
      "start_offset": 4,
      "end_offset": 6,
      "type": "word",
      "position": 1
    },
    {
      "token": "역",
      "start_offset": 6,
      "end_offset": 7,
      "type": "word",
      "position": 2
    },
    {
      "token": "에서",
      "start_offset": 7,
      "end_offset": 9,
      "type": "word",
      "position": 3
    },
    {
      "token": "롯데",
      "start_offset": 10,
      "end_offset": 12,
      "type": "word",
      "position": 4
    },
    {
      "token": "타워",
      "start_offset": 12,
      "end_offset": 14,
      "type": "word",
      "position": 5
    },
    {
      "token": "가",
      "start_offset": 14,
      "end_offset": 15,
      "type": "word",
      "position": 6
    },
    {
      "token": "보여요",
      "start_offset": 16,
      "end_offset": 19,
      "type": "word",
      "position": 7,
      "positionLength": 2
    },
    {
      "token": "보이",
      "start_offset": 16,
      "end_offset": 19,
      "type": "word",
      "position": 7
    },
    {
      "token": "어요",
      "start_offset": 16,
      "end_offset": 19,
      "type": "word",
      "position": 8
    }
  ]
}
```
⇒ 우와! 는 지움

## 6-1-3. nori_readingform 토큰 필터
```bash
# 지우고 다시 만들기
DELETE /article

# 먼저 커스텀하고 my_analyzer에 넣음
PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                //"user_dictionary": "custom/user_dict.txt"
                "user_dictionary_rules": [
                    "c++",
                    "씨쁠쁠"
                ]
            }
        },
        "filter": {
            "my_filter": {
                "type": "nori_part_of_speech",
                "stoptags" :[
                    "IC"
                ]
            }
        },
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer",
                "filter": [
                    "my_filter",
                    "nori_readingform"
                ]
            }
        }
    }
  }
}
```

- 테스트
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "大韓民國 세종시"
}
```
```bash
{
  "tokens": [
    {
      "token": "대한",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "민국",
      "start_offset": 2,
      "end_offset": 4,
      "type": "word",
      "position": 1
    },
    {
      "token": "세",
      "start_offset": 5,
      "end_offset": 6,
      "type": "word",
      "position": 2
    },
    {
      "token": "종시",
      "start_offset": 6,
      "end_offset": 8,
      "type": "word",
      "position": 3
    }
  ]
}
```
⇒ 한자 분석

## 6-1-4. nori_number 토큰 필터
```bash
# 지우고 다시 만들기
DELETE /article

# 먼저 커스텀하고 my_analyzer에 넣음
PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                //"user_dictionary": "custom/user_dict.txt"
                "user_dictionary_rules": [
                    "c++",
                    "씨쁠쁠"
                ]
            }
        },
        "filter": {
            "my_filter": {
                "type": "nori_part_of_speech",
                "stoptags" :[
                    "IC"
                ]
            }
        },
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer",
                "filter": [
                    "my_filter",
                    "nori_readingform",
                    "nori_number"
                ]
            }
        }
    }
  }
}
```

- 테스트
```bash
GET /article/_analyze
{
  "analyzer": "my_analyzer",
  "text": "大韓民國 세종시 143번지"
}
```
```bash
{
  "tokens": [
    {
      "token": "대한",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "민국",
      "start_offset": 2,
      "end_offset": 4,
      "type": "word",
      "position": 1
    },
    {
      "token": "세",
      "start_offset": 5,
      "end_offset": 6,
      "type": "word",
      "position": 2
    },
    {
      "token": "종시",
      "start_offset": 6,
      "end_offset": 8,
      "type": "word",
      "position": 3
    },
    {
      "token": "143",
      "start_offset": 9,
      "end_offset": 12,
      "type": "word",
      "position": 4
    },
    {
      "token": "번지",
      "start_offset": 12,
      "end_offset": 14,
      "type": "word",
      "position": 5
    }
  ]
}
```

## 6-1-5. 불용어, 동의어 설정
### 불용어
- `/elasticsearch-8.18.0/config/custom/stop.txt` 폴더 생성
```bash
우와
꺼져
바보
```

### 동의어
- 영어로 입력해도 한글까지 적용
- `/elasticsearch-8.18.0/config/custom/synonym.txt` 폴더 생성
```bash
ipad, 아이패드, i-pad, IPAD
python, 파이썬
personal computer => pc
```

- filter 설정
```bash
PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                //"user_dictionary": "custom/user_dict.txt"
                "user_dictionary_rules": [
                    "c++",
                    "씨쁠쁠"
                ]
            }
        },
        "filter": {
            "my_filter": {
                "type": "nori_part_of_speech",
                "stoptags" :[
                    "IC"
                ]
            },
            "my_stop_filter": {
                "type": "stop",
                "stopwords_path": "custom/stop.txt"
            },
            "my_synonym_filter": {
                "type": "synonym",
                "synonyms_path": "custom/synonym.txt"
            }
        },
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer",
                "filter": [
                    "my_filter",
                    "nori_readingform",
                    "nori_number",
                    "my_stop_filter",
                    "my_synonym_filter"
                ]
            }
        }
    }
  }
}
```

- 테스트 1 ⇒ `python`을 영어뿐만 아니라 `파이썬`인 한글로도 인식
```bash
GET /article/_analyze
{
    "analyzer": "my_analyzer",
    "text": "python good"
}
```
```bash
{
  "tokens": [
    {
      "token": "python",
      "start_offset": 0,
      "end_offset": 6,
      "type": "word",
      "position": 0
    },
    {
      "token": "파이썬",
      "start_offset": 0,
      "end_offset": 6,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "good",
      "start_offset": 7,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}
```

- 테스트 2 ⇒ `바보`는 불용어처리돼서 사라짐
```bash
GET /article/_analyze
{
    "analyzer": "my_analyzer",
    "text": "바보야 python이 c++보다 더 쉬워!"
}
```
```bash
{
  "tokens": [
    {
      "token": "야",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 1,
      "positionLength": 2
    },
    {
      "token": "이",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 1
    },
    {
      "token": "야",
      "start_offset": 2,
      "end_offset": 3,
      "type": "word",
      "position": 2
    },
    {
      "token": "python",
      "start_offset": 4,
      "end_offset": 10,
      "type": "word",
      "position": 3
    },
    {
      "token": "파이썬",
      "start_offset": 4,
      "end_offset": 10,
      "type": "SYNONYM",
      "position": 3
    },
    {
      "token": "2",
      "start_offset": 10,
      "end_offset": 11,
      "type": "word",
      "position": 4
    },
    {
      "token": "c++",
      "start_offset": 12,
      "end_offset": 15,
      "type": "word",
      "position": 5
    },
    {
      "token": "보다",
      "start_offset": 15,
      "end_offset": 17,
      "type": "word",
      "position": 6
    },
    {
      "token": "더",
      "start_offset": 18,
      "end_offset": 19,
      "type": "word",
      "position": 7
    },
    {
      "token": "쉬워",
      "start_offset": 20,
      "end_offset": 22,
      "type": "word",
      "position": 8,
      "positionLength": 2
    },
    {
      "token": "쉽",
      "start_offset": 20,
      "end_offset": 22,
      "type": "word",
      "position": 8
    },
    {
      "token": "어",
      "start_offset": 20,
      "end_offset": 22,
      "type": "word",
      "position": 9
    }
  ]
}
```

- 테스트 3 ⇒ `ipad`를 `i`와 `pad`로 쪼개고 `IPAD`도 적용
```bash
GET /article/_analyze
{
    "analyzer": "my_analyzer",
    "text": "ipad를 구매했습니다. 너무 비싸요ㅠㅠ"
}
```
```bash
{
  "tokens": [
    {
      "token": "ipad",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 0
    },
    {
      "token": "i",
      "start_offset": 0,
      "end_offset": 4,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "IPAD",
      "start_offset": 0,
      "end_offset": 4,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "를",
      "start_offset": 4,
      "end_offset": 5,
      "type": "word",
      "position": 1
    },
    {
      "token": "pad",
      "start_offset": 4,
      "end_offset": 5,
      "type": "SYNONYM",
      "position": 1
    },
    {
      "token": "구매",
      "start_offset": 6,
      "end_offset": 8,
      "type": "word",
      "position": 2
    },
    {
      "token": "했",
      "start_offset": 8,
      "end_offset": 9,
      "type": "word",
      "position": 3,
      "positionLength": 2
    },
    {
      "token": "하",
      "start_offset": 8,
      "end_offset": 9,
      "type": "word",
      "position": 3
    },
    {
      "token": "았",
      "start_offset": 8,
      "end_offset": 9,
      "type": "word",
      "position": 4
    },
    {
      "token": "습니다",
      "start_offset": 9,
      "end_offset": 12,
      "type": "word",
      "position": 5
    },
    {
      "token": "너무",
      "start_offset": 14,
      "end_offset": 16,
      "type": "word",
      "position": 6
    },
    {
      "token": "비싸",
      "start_offset": 17,
      "end_offset": 19,
      "type": "word",
      "position": 7
    },
    {
      "token": "요",
      "start_offset": 19,
      "end_offset": 20,
      "type": "word",
      "position": 8
    },
    {
      "token": "ㅠㅠ",
      "start_offset": 20,
      "end_offset": 22,
      "type": "word",
      "position": 9
    }
  ]
}
```

- mapping 설정
```bash
DELETE /article

PUT /article
{
  "settings": {
    "analysis": {
        "tokenizer": {
            "my_tokenizer": {
                "type": "nori_tokenizer",
                "decompound_mode": "mixed",
                //"user_dictionary": "custom/user_dict.txt"
                "user_dictionary_rules": [
                    "c++",
                    "씨쁠쁠"
                ]
            }
        },
        "filter": {
            "my_filter": {
                "type": "nori_part_of_speech",
                "stoptags" :[
                    "IC"
                ]
            },
            "my_stop_filter": {
                "type": "stop",
                "stopwords_path": "custom/stop.txt"
            },
            "my_synonym_filter": {
                "type": "synonym",
                "synonyms_path": "custom/synonym.txt"
            }
        },
        "analyzer": {
            "my_analyzer": {
                "type": "custom",
                "tokenizer": "my_tokenizer",
                "filter": [
                    "my_filter",
                    "nori_readingform",
                    "nori_number",
                    "my_stop_filter",
                    "my_synonym_filter"
                ]
            }
        }
    }
  },
  "mappings": {
    "properties": {
      "content": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}
```

- `article`테이블에 문서 만들기
```bash
POST /article/_doc
{
  "content": "바보야 c++이 파이썬보다 더 어렵잖아"
}

POST /article/_doc
{
  "content": "python 오늘 처음 배웠어요"
}
```

- python 검색 ⇒ 영어로 검색했지만 한국어도 검색함
```bash
GET /article/_search?q=content:python
```
```bash
{
  "took": 25,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 2,
      "relation": "eq"
    },
    "max_score": 0.27967125,
    "hits": [
      {
        "_index": "article",
        "_id": "uynFjpYBFidC3kFCcdH0",
        "_score": 0.27967125,
        "_source": {
          "content": "python 오늘 처음 배웠어요"
        }
      },
      {
        "_index": "article",
        "_id": "uinFjpYBFidC3kFCa9HS",
        "_score": 0.25445884,
        "_source": {
          "content": "바보야 c++이 파이썬보다 더 어렵잖아"
        }
      }
    ]
  }
}
```

# 6-2. 검색 결과 하이라이트
```bash
# 하이라이트
GET /article/_search
{
  "query": {
    "match": {
      "content": "python"
    }
  },
  "highlight": {
    "fields": {
      "content": {}
    }
  }
}
```
```bash
{
  "took": 47,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 2,
      "relation": "eq"
    },
    "max_score": 0.27967125,
    "hits": [
      {
        "_index": "article",
        "_id": "uynFjpYBFidC3kFCcdH0",
        "_score": 0.27967125,
        "_source": {
          "content": "python 오늘 처음 배웠어요"
        },
        "highlight": {
          "content": [
            "<em>python</em> 오늘 처음 배웠어요"
          ]
        }
      },
      {
        "_index": "article",
        "_id": "uinFjpYBFidC3kFCa9HS",
        "_score": 0.25445884,
        "_source": {
          "content": "바보야 c++이 파이썬보다 더 어렵잖아"
        },
        "highlight": {
          "content": [
            "바보야 c++이 <em>파이썬</em>보다 더 어렵잖아"
          ]
        }
      }
    ]
  }
}
```
⇒ `python`과 `파이썬`을 em태그로 감싸서 검색 결과 출력