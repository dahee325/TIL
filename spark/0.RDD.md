# 0. RDD
- Spark RDD(Resilient Distributed Dataset)
- 분산된 불변의 데이터 집합 -> spark에서 가장 기본이 되는 구조
- `localhost:8080` 링크 접속 -> `Notebook 화살표` -> `Create new note` -> `0.RDD`이름 설정

## word_RDD
- `damf2/data/word.txt` 파일 생성
```txt
apple world
hello apple
world world world
apple hi
hihi hi
```

### 파일 불러오기
```python
%pyspark # %pyspark를 써야 zeppelin이 spark코드라는걸 인식함

# sc = SparkContext()

# 로컬 파일 읽기
file_path = 'file:///home/ubuntu/damf2/data/word.txt' # 파일이면 `file://`이라고 알려줘야함
lines = sc.textFile(file_path) # 지정한 경로의 파일을 읽어서 RDD로 반환
# print(lines.collect()) #  RDD의 모든 데이터를 드라이버로 모아서 리스트로 반환
```
![alt text](/spark/assets/word.png)

### 하둡 실행
```shell
~/hadoop-3.3.6/sbin/start-all.sh
```

### 하둡에 파일 올리기
```shell
hdfs dfs -put ~/damf2/data/word.txt /input
# 확인
hdfs dfs -ls /input
```

### HDFS에서 파일읽기
```python
%pyspark

file_path = 'hdfs://localhost:9000/input/word.txt'
lines = sc.textFile(file_path)
print(lines.collect())
```

### 띄어쓰기를 기준으로 분리-> 일회용함수 lambda 사용
```python
words = lines.flatMap(lambda line: line.split())
print(words.collect())
```

### mapreduce 과정 중 map을 끝낸 결과
```python
mapped_words = words.map(lambda word: (word, 1))
print(mapped_words.collect())
```

### mapreduce 과정 중 reduce 
```python
# reduceByKey() : 키를 바탕으로 reduce해줌
reduced_words = mapped_words.reduceByKey(lambda a, b: a+b)
print(reduced_words.collect())
```

## log RDD
### 파일 불러오기
- `2024-01-01.logs` 로그 파일
```python
%pyspark

file_path = 'file:///home/ubuntu/damf2/data/logs/2024-01-01.log'
lines = sc.textFile(file_path)
print(lines.collect())
```

### 각 데이터 한줄씩 쪼개기 -> 한줄씩 의미가 있기 때문
```python
mapped_lines = lines.map(lambda line: line.split())
print(mapped_lines.collect())
```

### 4XX status code 필터링 -> 4로 시작하는 line데이터 출력
```python
def filter_4xx(line):
    return line[5][0] == '4'

filtered_lines = mapped_lines.filter(filter_4xx)
print(filtered_lines.collect())
```

### method('GET', 'POST') 별 요청수 계산
```python
# map, reduce
method_rdd = mapped_lines.map(lambda line: (line[2], 1)).reduceByKey(lambda a, b: a+b)
print(method_rdd.collect())
```

### 시간대별 요청수 
```python
# :을 기준으로 line[1]데이터 쪼개기
time_rdd = mapped_lines.map(lambda line: (line[1].split(':')[1], 1)).reduceByKey(lambda a, b: a+b)
print(time_rdd.collect())
```

### status code, api method 별 count 
- 그룹으로 묶기
```python
# status code, api method 별 count
# 2개의 데이터를 넣기 위해서 튜플 안에 튜플을 넣음
count_rdd = mapped_lines.map(lambda line: ((line[5], line[2]), 1)).reduceByKey(lambda a, b: a+b) 
print(count_rdd.collect())
```

## [Mockaroo](https://www.mockaroo.com/)
### 가상의 파일 다운
- 홈페이지 -> rows 50, format : CSV로 설정 -> `PREVIEW`누르면 가상의 데이터 만들어진걸 미리 볼 수 있음 -> `GENERATE DATA`파일 다운 -> `user`로 파일 이름 변경
![alt text](/spark/assets/user.png)
- 새로운 가상의 파일 생성 -> `post`로 파일 이름 변경
![alt text](/spark/assets/post.png)
- `/home/ubuntu/damf2/data`로 `user.csv`와 `post.csv`파일 위치 이동

### 파일 불러오기
```python
%pyspark

user_file_path = 'file:///home/ubuntu/damf2/data/user.csv'
post_file_path = 'file:///home/ubuntu/damf2/data/post.csv'

user_lines = sc.textFile(user_file_path)
post_lines = sc.textFile(post_file_path)

# print(user_lines.collect())
```

### ,를 기준으로 한줄씩 쪼개기
```python
user_rdd = user_lines.map(lambda line: line.split(','))
post_rdd = post_lines.map(lambda line: line.split(','))
print(post_rdd.collect())
```

### (user_id, user) -> user_id를 밖으로 뺀 후 튜플로 묶기
```python
user_tuple = user_rdd.map(lambda user: (user[0], user)) # user_id가 0번째에 위치
print(user_tuple.collect())
```

### (user_id, post)
```python
post_tuple = post_rdd.map(lambda post: (post[1], post)) # user_id가 1번째에 위치
```

### join
```python
# 첫 변수를 기준으로 join해줌
# 기본적으로 공통적으로 가진 데이터만 출력 -> 게시물을 작성하지 않은 사람은 빠짐
joined_rdd = user_tuple.join(post_tuple)
print(joined_rdd.collect())
```
![SQL join](/spark/assets/sql_join.png)