### 기본 실행
- 하둡 실행
```shell
hadoop-3.3.6/sbin/start-all.sh
```
- 제플린 실행 => `localhost:8080`
```shell
zeppelin-0.11.2/bin/zeppelin-daemon.sh start
```

### download
- fish 파일 download -> 머신러닝 배울 때 사용했던 파일
```shell
wget https://gist.githubusercontent.com/5chang2/6923330a794da4f4e05c06599c592914/raw/8ffaa770a44541cded1146b524820ef6c50856ec/fish.csv
```
- 하둡에 업로드
```shell
hdfs dfs -put fish.csv /input
```

## zeppelin
- 파일 가져오기
```python
%pyspark
# file_path = 'file:///home/ubuntu/damf2/data/fish.csv'
file_path = 'hdfs://localhost:9000/input/fish.csv'

df = spark.read.csv(file_path, header=True, inferSchema=True)
```

- 확인
```python
%pyspark
df.show()
```

- 데이터 유형 확인
```python
%pyspark
df.printSchema()
```

## features를 기준으로 Weight 예측
### 선형회귀분석
- `StringIndexer()` : 문자형인 데이터를 숫자형으로 변환
```python
%pyspark
from pyspark.ml.feature import StringIndexer # 문자를 숫자로 바꿔줌

# inputCols=[''] : 숫자로 바꿀 변수, outputCols=[''] : 숫자로 바꾼 값들을 저장할 변수
indexer = StringIndexer(inputCols=['Species'], outputCols=['species_idx'])
df = indexer.fit(df).transform(df)
```

- 확인
```python
%pyspark
z.show(df)
```

- `species_idx` 원핫인코딩 처리
```python
%pyspark
from pyspark.ml.feature import OneHotEncoder

encoder = OneHotEncoder(inputCols=['species_idx'], outputCols=['species_ohe'])

df = encoder.fit(df).transform(df)
```

- 확인 ⇒ `(6,[],[])`은 `000000`을 의미
```python
%pyspark
df.show()
```

- 예측에 필요한 변수들을 하나의 벡터로 만들기
```python
%pyspark
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(
    inputCols=['species_ohe', 'Length1', 'Length2', 'Length3','Height', 'Width'], 
    outputCol='features')
    
df = assembler.transform(df)
```

- 데이터 분리 -> 훈련 데이터, 테스트 데이터
```python
%pyspark
train_data, test_data = df.randomSplit([0.8, 0.2])
```

- train_data 학습
```python
%pyspark
from pyspark.ml.regression import LinearRegression
# 선형회귀
# featuresCol='', 기준이 되는 컬럼, labelCol='', 결과를 담을 컬럼
lr = LinearRegression(featuresCol='features', labelCol='Weight')
lr_model = lr.fit(train_data) # 학습
```

- test_data 예측
```python
%pyspark
prediction = lr_model.transform(test_data)
```

- 모델 평가
```python
%pyspark
# 실제값과 예측한 값이 얼만큼 거리가 떨어져있는지
from pyspark.ml.evaluation import RegressionEvaluator
# labelCol='' : 실제값, predictionCol='' : 예측한 값
evaluator = RegressionEvaluator(labelCol='Weight', predictionCol='prediction', metricName='rmse')
result = evaluator.evaluate(prediction)

print(result) # => 140.1376
```