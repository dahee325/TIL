# 2. airline
## hadoop
### 하둡 실행
```shell
~/hadoop-3.3.6/sbin/start-all.sh
```

### zepplein 실행
```shell
 ~/zeppelin-0.11.2/bin/zeppelin-daemon.sh start
```

### `damf2/data/airline` 폴더생성 → 다운받은 `2007.csv`, `2008.csv`파일 `airline`폴더로 옮기기

### `damf2/data/airline/upload_to_hdfs.sh` 파일 생성 → 자동화 시키기
```sh
 for year in $(seq 2007 2008)
do
    hdfs dfs -put $year.csv /input/airline # $year : year가 변수임을 알려줌
done
```

### 하둡에 upload
```shell
# 위치 이동
cd damf2/data/airline/

# 하둡에 airline 폴더 생성
hdfs dfs -mkdir /input/airline

# 자동화 실행
sh upload_to_hdfs.sh
```

## Zepplien
### 파일 불러오기
```python
%pyspark

file_path = 'hdfs://localhost:9000/input/airline'

# header=True : 첫번째줄은 컬럼이름이라고 알려주는 옵션
# inferSchema=True : integer가 나을 것 같다고 판단되는 컬럼을 integer로 형변환해주는 옵션
df = spark.read.csv(file_path, header=True, inferSchema=True)
```

### 확인
```python
%pyspark
df.printSchema()
```

### 데이터 수 세기
```python
%pyspark
df.count()
```

### 시각화 함수
- zepplein 에서 시각화 할 수 있는 함수 → 테이블, 그래프 등을 볼 수 있음
```python
%pyspark
z.show(df)
```
![alt text](/spark/assets/z_show.png)

### 필요한 변수 선택
```python
%pyspark
from pyspark.sql.functions import col

df = df.select(
    'Month',
    'DayofMonth',
    'DayOfWeek',
    'Origin',
    'Dest',
    'Cancelled',
    'UniqueCarrier',
    'Distance',
    col('AirTime').cast('int'),
    col('ArrTime').cast('int'),
    col('ArrDelay').cast('int'),
    col('DepTime').cast('int'),
    col('DepDelay').cast('int'),
    col('ActualElapsedTime').cast('int'),
    col('CRSElapsedTime').cast('int'),
)
```

### 확인
```python
%pyspark
df.printSchema()
```

## sql & pyspark
### table로 변환
```python
%pyspark
df.createOrReplaceTempView('airline')
```

### 데이터 출력
```sql
%sql
SELECT * FROM airline LIMIT 10;
```
```python
%pyspark
z.show(df.limit(10))
```

### 전체 데이터 개수 출력
```sql
%sql
SELECT COUNT(*) FROM airline;
```
```python
%pyspark
df.count()
```

### 항공사 종류
```sql
%sql
SELECT DISTINCT UniqueCarrier
FROM airline
ORDER BY UniqueCarrier;
```
```python
%pyspark
df.select('UniqueCarrier').distinct().orderBy('UniqueCarrier').show()
```

### 항공사별 수
```sql
%sql
SELECT UniqueCarrier, COUNT(*)
FROM airline
GROUP BY UniqueCarrier;
```
```python
%pyspark
from pyspark.sql.functions import *

df.groupBy('UniqueCarrier').agg(count('*')).show()
```

### 요일별 출발/도착 지연 평균
```sql
%sql
SELECT DayOfWeek, AVG(DepDelay), AVG(ArrDelay)
FROM airline
GROUP BY DayOfWeek
ORDER BY DayOfWeek;
```
```python
%pyspark
df.groupBy('DayOfWeek').agg(avg('DepDelay'), avg('ArrDelay')).show()
```

### 항공사별, 월별 지연 및 운항 건수
```sql
%sql
SELECT UniqueCarrier, Month, COUNT(*), AVG(DepDelay)
-- SELECT 
		UniqueCarrier, 
		Month, 
		SUM(CASE WHEN DepDelay > 0 OR ArrDelay > 0 THEN 1 ELSE 0 END) AS DelayFlights, 
		COUNT(*)
FROM airline
WHERE Cancelled = 0 
GROUP BY UniqueCarrier, Month;
```
```python
%pyspark
df.groupBy('UniqueCarrier', 'Month').agg(count('*'), avg('DepDelay')).show()
# df.filter(df.Cancelled == 0
#     ).groupBy('UniqueCarrier', 'Month'
#     ).agg(
#         sum(when(
#                 (df.DepDelay > 0) | (df.ArrDelay > 0), 1
#             ).otherwise(0)).alias('DelayFlights'), 
#         count('*')
#     ).show()
```

### 항공사별 취소율(=취소된 횟수/계획 비행횟수)
```sql
%sql
-- SELECT Uniquecarrier, AVG(Cancelled)
SELECT Uniquecarrier, SUM(Cancelled)/COUNT(*)
FROM airline
GROUP BY UniqueCarrier;
```
```python
%pyspark
df.groupBy('UniqueCarrier').agg(sum('Cancelled')/count('*')).show()
```

### 취소된 비행과 운항된 비행 분리해서 실행
```sql
%sql
SELECT
    *, 
    (cancelled_flight_count / total_count * 100) AS cancel_rate
FROM
(SELECT 
    Uniquecarrier, 
    SUM(Cancelled) AS cancelled_flight_count, --취소된 비행
    SUM(CASE WHEN Cancelled == 0 THEN 1 ELSE 0 END), --운행된 비행
    COUNT(*) AS total_count
FROM airline
GROUP BY UniqueCarrier)
```
```python
%pyspark
df.groupBy('UniqueCarrier')\
    .agg(
        sum('Cancelled').alias('cancelled_flight_count'), 
        sum(when(df.Cancelled == 0, 1).otherwise(0)), 
        count('*').alias('total_count')
    ).withColumn('cancel_rate', 
        col('cancelled_flight_count') / col('total_count')  * 100
    ).show()
```

### 가장 붐비는 공항
```sql
%sql
SELECT *, origin_count + dest_count AS total_count
FROM
(
(SELECT Origin, COUNT(*) AS origin_count
FROM airline
GROUP BY Origin) AS origin_airline

JOIN

(SELECT Dest, COUNT(*) AS dest_count
FROM airline
GROUP BY Dest) AS dest_airline

ON origin_airline.Origin == dest_airline.Dest
)
ORDER BY total_count DESC;
```
```python
%pyspark
origin_df = df.groupBy('Origin').count()

dest_df = df.groupBy('Dest').count()

origin_df.join(dest_df, origin_df.Origin == dest_df.Dest)\
    .withColumn(
        'total_count', 
        origin_df['count'] + dest_df['count']
    ).orderBy(desc('total_count')
    ).show()
```
### 실제 비행시간 / 예상 비행시간 차이가 큰 비행노선
- version 1
```sql
%sql
SELECT
    Month,
    DayofMonth,
    ABS(ActualElapsedTime-CRSElapsedTime) AS diff
FROM airline
ORDER BY diff DESC;
```
```python
%pyspark
df.select('FlightNum', abs(col('ActualElapsedTime') - col('CRSElapsedTime')).alias('FlightTime')).orderBy('FlightTime', ascending=False).show()
```

- version 2
```sql
%sql
SELECT 
    *,
    abs(real_time - crs_time) AS diff_time

FROM
(SELECT 
    Origin,
    Dest,
    AVG(ActualElapsedTime) AS real_time,
    AVG(CRSElapsedTime) AS crs_time
FROM airline
GROUP BY Origin, Dest)
ORDER BY diff_time DESC
```
```python
%pyspark
df.groupBy('Origin', 'Dest') \
    .agg(
        avg('ActualElapsedTime').alias('real_time'),
        avg('CRSElapsedTime').alias('crs_time')
    ).withColumn('diff_time', abs(col('real_time')-col('crs_time'))
    ).orderBy(desc('diff_time')
    ).show()
```