# 1. DF (DataFrame)
- [spark dataframe](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)

### 파일 불러오기
```
%pyspark

file_path = 'file:///home/ubuntu/damf2/data/logs/2024-01-01.log'

df = spark.read.csv(file_path, sep=' ') # 띄어쓰기를 기준으로 분리, ""는 안에 띄어쓰기가 있어도 하나의 묶음으로 봄
df.show() # 해당 데이터프레임을 표로 만들어서 보여줌
```

### 1행 출력
```
%pyspark
df.show(1)
```

### 모든 컬럼 출력
```
%pyspark
df.columns
```

### 스키마 구조 출력
```
%pyspark
df.printSchema()
```

### 특정 컬럼 출력
```
%pyspark
df.select('_c0', '_c1').show()
```

### 3개 행 출력
```
%pyspark
df.take(3)
```

### pandas 형태의 dataframe으로 변환
- `pip install pandas` : 가상환경이 비활성화된 상태에서 설치
```
%pyspark
pd_df = df.toPandas()
```
```
pd_df['_c0']
pd_df[['_c0', '_c2']]
```

![pandas dataframe](/spark/assets/pandas_df.png)
### _c2만 출력
```
%pyspark
df.select(df._c2).show()
```

### _c2 분리
- `pip install pyspark`
```
# _c2는 3개의 변수값을 갖고있음 -> 쪼갤 필요가 있음

%pyspark
from pyspark.sql.functions import split, col
# _c2를 띄어쓰기를 기준으로 분리한 후 0번째 데이터를 이름이 method인 새로운 컬럼 생성
df = df.withColumn('method', split(col('_c2'), ' ').getItem(0))
df = df.withColumn('path', split(col('_c2'), ' ').getItem(1))
df = df.withColumn('protocal', split(col('_c2'), ' ').getItem(2))

df.show()
```

### method가 POST인 데이터만 출력
```
%pyspark
df.filter(df.method == 'POST').show()
```

### method별 데이터 개수
- groupby 사용
```
%pyspark
df.groupby('method').count().show()
```

### method와 _c3 별 min, max, mean
```
%pyspark
from pyspark.sql.functions import min, max, mean

df.groupby('method', '_c3').agg(min('_c4'), max('_c4'), mean('_c4')).show()
```
=> 더 정확하게 하려면 형변환하고 실행해야함
- _c4의 유형을 integer로 변경
```
%pyspark
from pyspark.sql.functions import min, max, mean

df = df.select('method', '_c3', col('_c4').cast('integer'))

# method와 _c3 별 min, max, mean
df.groupby('method', '_c3').agg(min('_c4'), max('_c4'), mean('_c4')).show()
```

## 데이터프레임 테이블로 변경
```
%pyspark

file_path = 'file:///home/ubuntu/damf2/data/logs/2024-01-01.log'

df = spark.read.csv(file_path, sep=' ')

# 데이터프레임안의 데이터를 sql에 접근 가능한 테이블로 바꾸고 테이블 이름을 logs라 설정
df.createOrReplaceTempView('logs')
```
- 확인
```
%pyspark
spark.sql('''
    SELECT * FROM logs
''').show()
```

### sql문을 사용하여 _c2 분리
```
%pyspark
df = spark.sql("""
    SELECT *, SPLIT(_c2, ' ')[0] AS method, SPLIT(_c2, ' ')[1] AS path, SPLIT(_c2, ' ')[2] AS protocal
    FROM logs
""")
df.show()
df.createOrReplaceTempView('logs2')
```

### _c3가 400인 데이터만 출력
```
%pyspark

spark.sql('''
    SELECT * FROM logs2
    WHERE _c3 = 400
''').show()
```

### _c3가 200이고 path가 product를 포함하는 데이터 출력
```
%pyspark

spark.sql('''
    SELECT * FROM logs2
    WHERE _c3 = 200 AND path LIKE '%product%'
''').show()
```

### method별 데이터 개수
```
%pyspark

spark.sql('''
    SELECT method, COUNT(*) FROM logs2
    GROUP BY method
''').show()
```