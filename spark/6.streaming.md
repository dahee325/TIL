# Streaming data
- 지속적으로 들어오는 실시간 데이터
- 로그 형태의 데이터 처리

## 0. 기본 환경 실행
- 제플린 실행
```shell
zeppelin-0.11.2/bin/zeppelin-daemon.sh start
```

- 하둡 실행
```shell
~/hadoop-3.3.6/sbin/start-all.sh
```

## 1. 실시간 데이터 입력받기
- [튜토리얼](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview)
![streaming_data](/spark/assets/streaming_data.png)

### 1-1. 기본 설정
- 스파크만 단독으로 사용할 때 'socket'을 사용
```python
%pyspark
from pyspark.sql.functions import current_timestamp

# 데이터 읽기
# 'http프로토콜' : 한쪽이 말하고나서 한쪽이 말하는 단방향적 통신
# 통신 방식, 'socket' : 양방향적 통신
# 'port' : 포트 설정, 내맘대로 설정 가능
lines = spark.readStream.format('socket') \
        .option('host', 'localhost') \
        .option('port', '9999') \
        .load()
        
# 데이터 고르기(가공)
# current_timestamp() : 현재 시간 출력
lines_with_time = lines.select(lines.value, current_timestamp())

# 데이터 저장
# 'path' : 어디에 데이터를 저장할지 경로 설정
# 'checkPointLocation' : 임시저장
query = lines_with_time.writeStream \
        .outputMode('append') \
        .format('csv') \
        .option('path', 'hdfs://localhost:9000/output/stream-test') \
        .option('checkPointLocation', 'hdfs://localhost:9000/output/stream-temp') \
        .start()
        
# 데이터 실행
# 위의 코드를 계속 실행시켜줌
query.awaitTermination()
```

### 1-2. 포트 실행
- nc라는 프로그램한테 9999포트를 계속 실행시켜달라는 명령어
```shell
nc -lk 9999
```
=> 명령어 실행 후 제플린 코드 실행

## 2. wordcount
- 단어를 보내주면 몇 개인지 세주는 프로그램

### 2-1. 기본 설정
```python
%pyspark
from pyspark.sql.functions import split, explode

# 입력받은 데이터를 한줄로 쪼개기
lines = spark.readStream.format('socket').option('host', 'localhost').option('port', '9999').load()

# 한줄에 여러개인 단어 쪼개기
words = lines.select(explode(split(lines['value'], ' ')))

# 단어 개수 세기
word_count = words.groupBy('col').count()

# trigger() : 단어를 몇초에 한번씩 쓸지 시간 설정
# queryName('word') : word라는 테이블에 저장
query = word_count.writeStream \
        .trigger(processingTime='10 seconds') \
        .outputMode('complete') \
        .format('memory') \
        .queryName('word') \
        .start()
```

### 2-2. 포트 실행
```shell
nc -lk 9999
```
=> 제플린 실행 => 데이터 입력

### 2-3. 확인
```python
%pyspark
df = spark.table('word')
df.show()
```
- 쿼리 멈추기
```python
%pyspark
query.stop()
```