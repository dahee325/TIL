# 5. recommendation
### 파일 불러오기
```python
%pyspark
base_path = 'hdfs://localhost:9000/input/netflix/'

movie_data = spark.read.csv(base_path + 'Movie.csv', header=True, inferSchema=True)
rating_data = spark.read.csv(base_path + 'Rating.csv', header=True, inferSchema=True)
```

# ALS
- 협업필터링 기법 중 하나
- 행렬 분해 방식의 추천 알고리즘

- 모델 생성
    - `coldStart` : 훈련 데이터에 존재하지 않는 사용자나 아이템에 대해 모델이 평점을 예측할 수 없는 상황
    - `coldStartStrategy='drop'` : 결측값이 있으면 제거
```python
%pyspark
from pyspark.ml.recommendation import ALS
als = ALS(userCol='User_ID', itemCol='Movie_ID', ratingCol='Rating', coldStartStrategy='drop')
```

- 데이터 분리
```python
%pyspark
train_data, test_data = rating_data.randomSplit([0.8, 0.2])
```

- train_data 학습
```python
%pyspark
als_model = als.fit(train_data)
```

- test_data 예측
```python
%pyspark
prediction = als_model.transform(test_data)
```

### 사용자별 리뷰를 남긴 개수
```python
%pyspark
user_review_count = rating_data.groupBy('User_ID').count()
user_review_count.show()
```

## 1. 사용자에게 영화 추천
### 리뷰를 많이 남긴 사용자 top 5
```python
%pyspark
top_5_users = user_review_count.orderBy('count', ascending=False).limit(5)
top_5_users.show()
```

### 리뷰를 많이 남긴 사용자 top5에게 3개의 영화 추천
- 사용자가 추천할 해당 영화를 본다면 평점을 몇 점 남길지 예측한 후 상위 3개를 추천
```python
%pyspark
# recommendForUserSubset(subset, n) : 사용자의 부분집합(리뷰top5(subset))에 n개의 영화추천

usersubset = top_5_users.select('User_ID')
recommend = als_model.recommendForUserSubset(usersubset, 3)
# truncate=False : 축약하지 않고 출력
recommend.show(truncate=False)
```

### 리뷰를 많이 남긴 사용자 top5별 평균 평점
- `rating_data`와 `top_5_users`테이블 조인
```python
%pyspark
top_5_reviews = rating_data.join(top_5_users, on='User_ID')
top_5_reviews.show()
```
- 사용자별 평균 평점
```python
%pyspark
from pyspark.sql.functions import avg

top_5_reviews.groupBy('User_ID').agg(avg('Rating')).show()
```

## 2. 모든 사용자에 대해 영화 추천
### 모든 사용자에게 영화 3개 추천
```python
%pyspark
all_user_recommend = als_model.recommendForAllUsers(3)
all_user_recommend.show(truncate=False)
```

### 추천한 영화 리스트안의 여러개의 데이터 쪼개기
```python
%pyspark
# 리스트안의 여러개의 데이터를 낱개로 쪼갬 -> 리스트 안에 4개의 딕셔너리가 있으면 1개씩 4개의 딕셔너리로 쪼갬
from pyspark.sql.functions import explode

all_user_recommend_flat = all_user_recommend.withColumn('exp', explode('recommendations'))
all_user_recommend_flat.show()
```

### 쪼갠 딕셔너리 안의 데이터 출력
```python
%pyspark
result = all_user_recommend_flat.select('User_ID', 'exp.Movie_ID', 'exp.Rating')
result.show()
```

### 추천 영화의 제목 출력
```python
%pyspark
result.join(movie_data, on='Movie_ID').show()
```

## 3. 영화에 대한 사용자 추천
### Movie_ID가 3456인 영화를 사용자 5명에게 추천
```python
%pyspark

movie_subset = train_data.filter(train_data['Movie_ID'] == 3456)
lost_movie_user = als_model.recommendForItemSubset(movie_subset, 5)
lost_movie_user.show(truncate=False)
```

## 4. 사용자간 유사도
- 코사인 유사도 사용 -> 두 벡터가 얼마나 비슷한지

### 특정 사용자의 벡터 정보
```python
%pyspark
# 특정 유저의 회귀식 계수

user_factor = als_model.userFactors
user_factor.show(truncate=False)
```

### Movie_ID가 3456인 영화를 추천받은 사람에 대한 유사도
- 필요한 데이터 선택
```python
%pyspark
user1_vector = user_factor.filter(user_factor['id'] == '1848502').select('features').head()[0]
user2_vector = user_factor.filter(user_factor['id'] == '808181').select('features').head()[0]
print(user1_vector)
print(user2_vector)
```

- 숫자 벡터로 바꾸기
```python
%pyspark
from pyspark.ml.linalg import DenseVector # 숫자를 벡터로 바꿔줌

user1_v = DenseVector(user1_vector)
user2_v = DenseVector(user2_vector)
print(user1_v)
print(user2_v)
```

- 두 벡터의 내적
```python
%pyspark
# dot() : 벡터의 내적(곱)
user_1_2_dot = user1_v.dot(user2_v)
print(user_1_2_dot)
```

- (코사인)유사도
```python
# 유클리드 거리
user1_norm = user1_v.norm(2)
user2_norm = user2_v.norm(2)

# (코사인)유사도
Similarity = user_1_2_dot / (user1_norm * user2_norm)
# print(Similarity)
```

### 두 사용자가 평점 5점 준 영화 출력
```python
%pyspark
user1_5star = rating_data.filter((rating_data['User_ID'] == 1848502) & (rating_data['Rating'] == 5))
user2_5star = rating_data.filter((rating_data['User_ID'] == 808181) & (rating_data['Rating'] == 5))
```

### 두 사용자가 평점 5점 준 영화의 교집합 출력
```python
%pyspark
# 두 사용자가 평점 5점 준 영화의 교집합 출력
user_5star = user1_5star.join(user2_5star, on='Movie_ID')
# user_5star.show()

# 교집합의 영화 이름까지 출력
user_5star.join(movie_data, on='Movie_ID').show()
```

### 예측 평가
```python
%pyspark
from pyspark.ml.evaluation import RegressionEvaluator
# 거리가 얼만큼 떨어져있는지 제곱근으로 계산
evaluator = RegressionEvaluator(labelCol='Rating', predictionCol='prediction', metricName='rmse')
result = evaluator.evaluate(prediction)
print(result)
```

## 5. 모델 저장
### 하둡에 모델 저장
- 경로를 지정하면 자동으로 폴더를 만들어서 저장해줌
```python
%pyspark

als_model.save('hdfs://localhost:9000/input/netflix-model/')
```

### 저장한 모델 재사용
- 모델 불러오기
```python
%pyspark
from pyspark.ml.recommendation import ALSModel

load_model = ALSModel.load('hdfs://localhost:9000/input/netflix-model/')
```

- 모델 재사용
```python
%pyspark
load_model.recommendForAllUsers(3).show()
```